=> Preparing data..
=> Preparing data..
=> Building model..
=> Model Parameter: 11.690 M, FLOPs: 1816.408M
=> Building model..
Accuracy before quantization: top1:  69.76  top5:  89.08
[<class 'torch.nn.modules.conv.Conv2d'>, <class 'torch.nn.modules.linear.Linear'>]
size: 23357824
Acc after 8-bit (4x) compression: 69.76 89.08 

Total number of channels:  5800
channels quantized: 2782, acc: (58.428, 81.766), compression: 4.5x
channels quantized: 3730, acc: (58.248, 81.646), compression: 5.0x
channels quantized: 4320, acc: (58.412, 81.76), compression: 5.5x
channels quantized: 4758, acc: (58.386, 81.762), compression: 6.0x
channels quantized: 5108, acc: (58.312, 81.728), compression: 6.5x
channels quantized: 5384, acc: (58.272, 81.704), compression: 7.0x
channels quantized: 5601, acc: (58.252, 81.662), compression: 7.5x
5601/5800 channels quantized to 4-bit; acc: (58.252, 81.662), compression: 7.5x

Accuracy after quantization: top1:  58.252  top5:  81.662
Best quantized model saved to ./imagenet/quantized/rn18_rev.pth
